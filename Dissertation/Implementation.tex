\graphicspath{ {./Images/} }
\chapter{Implementation}
\section{Overview}
\label{implementationOverview}

The approach I took to implementing both the game client and the computer player was based mostly on on-the-fly problem solving rather than meticulous preparation and design. This chapter is about the process of implementation - how the process went, what problems I encountered along the way, and how the final result was affected by those problems. For the computer player, it will also delve into the structure of the code, and how exactly my version of the Monte Carlo tree search process works.

Overall, implementing the computer player took far longer and was much more difficult than I had anticipated. In order to make it work, I had to do extensive refactoring on the game client/game logic code that I had already written. Besides that, the lack of existing literature on how to specifically implement MCTS for Risk resulted in me being somewhat stuck for a long period of time, unsure on what approach to take and how to create even a minimal version of MCTS that worked. The final result is essentially that minimal version - a computer player that runs MCTS successfully, but which is far too inefficient to be actually used to play the game. For example, one of the experiments run by Lim{\'e}r et al \cite{limer2020monte} used the values of \texttt{rollouts = 5000, depth = 6, c = 0.5}, which resulted in a winrate of 67\%. In my implementation, a single move with these parameters takes just under 3 minutes to execute, and does not converge on a good move, with all first order children of the root having between 100 and 200 playthroughs.

\section{Game Client Implementation}
\label{gameClientImplementation}
\subsection{Initial Implementation}
\label{initialImplementation}
The initial implementation process for the game client was rather straight-forward. The first thing to be implemented were territories and the ability to click on them and for the game to recognize that. Afterwards, I created the \texttt{GameState} class (which, at the time, included the functionality of both \texttt{GameState} and \texttt{GameMaster}) and made methods in this class be called on click of territories. Almost all interactions the human player has with the game include clicking on the territories. As the implementation continued, I added a few levels of indirection between registering the click and executing the action, such as class \texttt{PlayerActions} and the subclasses of \texttt{Player}, as described in section \ref{codeStructure}. This was followed by the implementation of a simple UI, which provided information for players about whose turn it was and which turn stage they were in, along with a button to end their turn stage (for when a player wanted to stop attacking, or skip their one reinforcing move). This set of features encompasses most actions that a player can take and information that they need. The remaining work was on UI elements to interact with cards, a main menu/setup page, and various improvements and bug fixes.

A stand-out part of the implementation, and one of the most time-consuming, was drawing the game map. I started out with a picture of the game map taken from the internet as a placeholder, however, it soon became apparent that this will be insufficient, as it made several features nearly impossible to implement - such as changing colors of individual territories, or highlighting them on click/hover. A custom-drawn map was needed, however, I have very little skill and experience in visual art. In the end, I decided to use a reference picture of the game board of Risk, trace every territory, and export each territory as a separate image to be used as a sprite in the Unity game object that represents that territory. I used vector graphics in the drawing application Krita\footnote{Krita: https://krita.org/}, in order to have each border as a distinct, selectable line. This allowed me to avoid re-drawing a border multiple times and to make sure that, when the separate sprites are re-assembled into the finished map, they would match perfectly.

\subsection{Refactoring}
\label{refactoring}

In the implementation process, I essentially completed the game client before doing any substantial research into possible computer players or Monte Carlo tree search. This turned out to be a mistake, as during that research, it quickly became apparent that certain aspects of the implementation are incompatible with MCTS. The main problem was the structure of \texttt{GameState}, and how interconnected with other classes it was. There were many circular references - \texttt{GameState} kept track of all the players (and the subclasses of \texttt{Player} that represented them), however, each of those classes had a reference to \texttt{GameState} in order to get information about the state, such as who owns which territory. A similar circular reference existed between \texttt{GameState} and \texttt{Territory}, \texttt{Territory} and \texttt{Player}, and several others. This made cloning a game state impossible, but in order to run MCTS, a game state needs to be stored in each node, therefore requiring cloning.

In order to fix the cloning issue, all circular references had to be removed. Some of them could be resolved by moving functionality from one class to another, and removing the reference on one side. However, some references could not be removed this way - for example, \texttt{Territory} needed to keep track of who owns said territory, as this reference was widely used in the code - when checking whether a player action was legal, when determining the display color of a territory, and for updating data in the \texttt{Player} class in methods where the only reference available was the territory. The \texttt{Player} class also needed to maintain a list of owned territories, in order to calculate the amount of troops they would receive at the start of each turn, and for computer players to know which territories they can make moves from. In these situations, I replaced one side of the reference with a string (of a player's name, for example), and then added methods to the \texttt{GameState} class to find a reference to an object by going through the list of all objects (all players/all territories/etc.) and finding the one with the matching name. This introduces a problem of efficiency when using these string lookups frequently, however, it at least makes it possible to run an algorithm such as Monte Carlo tree search.

During this process, I separated the \texttt{GameState} into multiple parts in order to make the code more structured and to make swapping between game states easier. The main parts were:
\begin{itemize}
\item \texttt{GameState}, which was partially converted to a facade, containing a reference to other classes which represented different aspects of a game's state, such as the map and the deck of territory cards.
\item \texttt{Map}, a class that keeps track of every territory on the map, as well as providing methods to read/alter the data it contains (such as the aforementioned string lookup)
\item \texttt{GameMaster}, a singleton class which contains a reference to a single game state - the current state being played. This class is used by subclasses of \texttt{PlayerAction} to invoke changes in the current game state without having a reference to that game state. The singleton nature of this class also makes it easy to run MCTS simulations on a state - all one needs to do is save the current state as a reference, clone it, set the clone as the current state in \texttt{GameMaster}, run the simulation, and after the simulation is complete, restore the current state to the previously saved reference. This ensures any actions taken during the simulation will be executed on the cloned, temporary version of the state.
\end{itemize}

After the refactoring, I had a representation of the game state that could be cloned, and could proceed with implementing the MCTS algorithm itself. This process successfully solved the issues that were preventing me from running MCTS completely, but it is not an efficient solution. The refactoring process also took multiple weeks, which is far longer than I expected. The inefficiency of the solution, combined with the amount of time it took to implement it, is one of the biggest reasons why the final implementation of the computer player performs poorly.

\section{Computer Player Implementation}
\label{computerPlayerImplementation}
\subsection{Code Structure}
\label{computerPlayerCodeStructure}

The code for the MCTS player mainly resides in the \texttt{MCTSPlayer} class. This class is a subclass of \texttt{Player}, which means it has access to some general player fields (such as name, type of player and hand of cards), and overrides some abstract methods of \texttt{Player}, mainly \texttt{StartTurn()}. This method is the starting point for a player's turn logic. In the case of \texttt{MCTSPlayer}, the method deploys troops using a rule-based approach, performs MCTS to find the chain of attacks it should execute, executes those attacks, and makes a rule-based reinforcing move.

There are several additional classes that help execute the Monte Carlo tree search algorithm. Firstly, \texttt{GameStateTreeNode} is a class that allows for the construction of the tree structure in MCTS. It is a modified implementation of the composite design pattern - there is no separate leaf class, as at any point during the construction of the tree a leaf may get children of its own. The class contains a single instance of \texttt{GameState} (the state represented by the node), a list of children \texttt{GameStateTreeNode}s, a reference to its parent (for backpropagation), and other helper information, such as number of playthroughs, cumulative heuristic value, and the "source move" - the move that created this game state from the game state of the parent node. These source moves are used when assembling the final chain of moves to be executed after MCTS is finished.

Another helper class is \texttt{Strategies}, a static class with methods that represent the rule-based strategies described in section \ref{ruleBasedLogic}. It was necessary to separate the strategies into a separate class because some of them were re-used when both making the real deploy or reinforce move and when running the MCTS simulation. This also allowed me to quickly compare different strategies, by simply changing with method from \texttt{Strategies} was being called.

\subsection{Rule-Based Logic}
\label{ruleBasedLogic}

The MCTS player uses rule-based approaches for deploying new troops and reinforcing existing territories. These approaches are re-used in the simulation part of the MCTS algorithm, as well as using another rule-based approach for simulating attacks. These strategies need to be computationally cheap in order to be used in the simulation, as well as reasonably correct so that they are not the bottleneck in the performance of the computer player. I experimented with several strategies for all of these parts of a turn, starting with fully random moves, and applying restrictions from there.

The final approach to deploying troops takes into account the player's strength in continents. Step-by-step, the strategy is:
\begin{enumerate}
\item Create 2 empty arrays, where an element represents the number of troops in a continent - one array for the total number of troops in the continent, and another for the number of troops a specific player owns in that continent.
\item Iterate through all territories on the map. For each territory, add the troop count to the total troops array, and if the territory is controlled by the player being simulated, add the troop count to the owned troops array. If the territory has neighbours in another continent, add the troop values of those territories as well - as troops that neighbour a continent can still threaten and attack that continent.
\item After populating the arrays, calculate the ratio of owned troops / total troops for each continent. This should provide a rating for how strong a player is in a certain continent - the higher the ratio, the stronger.
\item Select the continent with the highest ratio that is not equal to 1 (meaning, all territories in and near the continent are controlled).
\item From the territories in the selected continent, select the one that has the \textbf{lowest} ratio of own troops inside the territory / enemy troops neighbouring the territory.
\item Deploy all troops inside the selected territory.
\end{enumerate}

The strategy for making the 1 reinforcing move is simpler - find all territories that have more than 1 troop and are surrounded by friendly territories, from that list, get the territory with the most troops in it, and move those troops to the neighbour with the lowest troop ratio as described by step 5 in the list above.

Both of these strategies are used in the real deployment and reinforcement of troops, as well as in the MCTS simulation step. The simulation also requires a strategy for making the attacks. I started with the attacks in the simulation being completely random, however, this appeared to be insufficient, as simulations would never reach an end state in the game (even at extreme depths). My initial hypothesis was that, without trying to capture continents, no player would ever get an advantage - each player starts with the same number of troops and the same troop income, and the battles they fight are mostly balanced in terms of losses. The dice roll randomness would create some variance, but the rolls would average out in multiple moves.

To avoid this, I created a new attack strategy that took continents into account, similar to the deploy strategy above. However, in the process of implementing this, I discovered multiple critical bugs in the simulation code, meaning that the never-ending simulation was most likely the result of the simulation not properly running due to these bugs. I decided to use the newer version of the attack strategy anyway, as it is not too computationally expensive, and it is more reflective of a real strategy a player might employ. The precise simulation attack strategy is as follows:
\begin{enumerate}
\item For the player that is making the attack, find the strongest continent, as described in steps 1-4 of the deployment strategy.
\item Get all possible attacks on the map that the player can make. From this list, get the attacks whose target is in the strongest continent. If this filtered list is empty, reset the list to all possible attacks.
\item Add null to the list of attacks, which represents an "end turn now" action.
\item Select a random entry from the list of attacks.
\item Execute the selected until either full victory or defeat. If victory, move all the troops into the new territory.
\item Repeat until the selected entry is null.
\end{enumerate}

As said previously, this strategy is close to how a human player would play the game - deploying and attacking in the continent they are strongest in. It is also not very computationally expensive - the most expensive part is getting all possible attacks for a player, which would be required even if the strategy was "random attacks".