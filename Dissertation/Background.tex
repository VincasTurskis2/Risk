\graphicspath{ {./Images/} }
\chapter{Background}
\label{background}
\section{Rules of Risk}
\label{rulesOfRisk}
Risk \cite{riskrules} is a 2-6 player strategy board game, in which players try to conquer the world. The game is played on a game board representing the entire world, which is divided into 42 territories, with each of those territories belonging to one of 6 continents. Players start by controlling an equal amount of territories each, and must earn more troops and attack territories of other players in order to win.

A game starts by each player placing one troop on a territory, thereby claiming it. Once every territory is claimed, players continue placing troops on their already claimed territories, until each player has placed a predetermined number of troops, which can range from 40 to 20, depending on how many players there are. After this set-up phase is complete, players take their turns in sequence. A turn begins with the player distributing newly earned troops on their territories. The amount of new troops they earn is predetermined based on territories and continents - the player gets 1 troop for every 3 territories they control, rounded down, with a minimum of 3 troops, even if they control less than 9 territories. Furthermore, the player gets extra troops if they control every territory in a continent, with the exact number they get depending on the specific continent - ranging from 2 troops for controlling Australia up to 7 troops for controlling Asia.

After their new troops are deployed, the player can start making attacks. Territories can be attacked from adjacent territories that contain more than 1 troop. It is never allowed, under any circumstances, to leave a territory with 0 troops, therefore, for example, when trying to attack from a territory with 8 troops, only 7 troops can participate in the attack - there must always be at least 1 troop that is left in a territory. The attack itself is decided via dice roll. The attacker rolls 3 dice or less, if they have less than 3 attacking troops. The defended rolls 2 dice or less, if they have less than 2 defending troops. After the dice are rolled, the highest rolled numbers from both players are matched, and the second highest numbers from both players are matched. For each match, whoever has the higher roll makes the other lose 1 troop. Ties are won by the defender. For example, say an attacker rolled dice with values \texttt{5, 3, 2}, and the defender rolled \texttt{5, 1}. The highest rolls are a 5 for both players, so the 5 are matched against each other. The second highest rolls are 3 for the attacker, 1 for the defender. The first match is a tie, therefore, the defender wins, and the attacker loses 1 troop. In the second match, the attacker rolled higher, so the defender loses 1 troop. Final result: both players lose 1 troop. The attacker can choose to stop attacking after each dice roll is resolved, or can keep rolling dice until either player has no troops left to fight for that territory.

If an attack is successful - that is, if the defender loses all of their defending troops - the attacking player can move some of the troops they were attacking with into the newly captured territory. The minimum number of troops they have to move is the number of dice they rolled on their last attack (most likely, 3). The maximum number is the total number of attacking troops (keeping in mind that, once again, at least 1 troop must be left behind). The player can make any number of different attacks in their turn, until either there are no more legal attacks to make, or they decide to stop attacking. After a player decides to stop attacking, they can make one reinforcing move between 2 of their own (adjacent) territories, moving any number of troops between those two territories. Once this move is done, the turn ends, and the next player starts their turn - deploying new troops, attacking, then reinforcing their own territories.

If a player has captured at least 1 territory in their turn, at the end of said turn, they receive a card. There are 44 cards in total - 42 representing each territory on the board, and 2 "wild cards". Each non-wild card also has a picture on it - of either infantry, cavalry, or artillery. When deploying new troops at the start of their turn, a player may trade-in their cards for extra troops. They can trade in a set of 3 cards, which must have either matching pictures or 3 different pictures. A wild card may be used in place of any picture type. If a player has 5 or more cards at the start of their turn, they must trade a set in (as it is guaranteed that they have a valid set). The first set traded in gives 4 troops, and every set traded in by any player increases the reward of the next set to be traded in. Furthermore, if the player owns any of the territories represented by the cards that are being traded in, they receive 2 extra troops from that set. Finally, if a player is defeated - meaning they do not control any territories - their hand of cards is given to the player that captured their final territory.

\section{Monte Carlo Tree Search Theory}
\label{MCTSTheory}

Monte Carlo tree search (MCTS) \cite{Coulom2007MonteCarlo} is a search algorithm that - as the name suggests - utilizes the Monte Carlo method to explore a game tree. More specifically, MCTS builds a tree of moves by continually running simulations until a game concludes and keeping track of the results of those simulations, preferring to explore nodes that have a higher average simulation results. The MCTS algorithm starts from the current game state as the root of the tree, and generally has 4 stages:
\begin{itemize}
\item Select - the algorithm travels from the root to an unexplored leaf node. The chosen path/leaf node depends on the cumulative simulation values of the nodes, and how many times a node has been visited. The algorithm needs to strike a balance between exploration of nodes that have not been simulated many times, and exploitation of nodes that have high potential.
\item Expand - The algorithm creates a new leaf node from the selected node by picking a random move out of all the possible moves the player can make in that node and adding the game state resulting from that move as a new node
\item Simulate - From the newly expanded node, a simulation is run until the end of the game, and the result is recorded - 1 if the player won, 0 if they lost. For games with a depth too large to realistically simulate many times, the simulation can cut off at a certain depth and perform a heuristic evaluation, returning a value between 0 and 1.
\item Backpropagate - the result of the simulation is backpropagated up the tree. Starting from the new node - increase the "number of simulations" counter by 1, increase the cumulative simulation value by the result of the simulation (0/1/heuristic value), go to the parent of the node. Repeat until the root node is reached.
\end{itemize}

This process has several advantages over other search approaches such as minimax or alpha-beta pruning. The first one is that the "pure" version of MCTS (with simulating until game end, rather than cut-off and heuristic evaluation) requires no game knowledge - game states are evaluated based on how many simulations were won from that state, rather than a heuristic that is based on expert game knowledge. Some understanding of the game rules are required to make a viable simulation, however, the simulation can - and should - be very rudimentary, as it needs to be computationally cheap. For example, a possible strategy for simulations for chess could be "capture pieces when possible". This is useful, as games that are interesting enough for research into using search-based approaches are usually not solved - meaning, even the best expert human players do not have complete knowledge and do not play the game fully optimally.

The second advantage of MCTS is its ability to deal with games with a high branching factor. The MCTS algorithm focuses on nodes that have both a high cumulative simulation value (meaning the simulations that have gone through that node have a high chance of resulting in a win) and a high number of playthroughs (meaning the algorithm has a high certainty for the accuracy of the simulation value). The algorithm will usually follow a course of exploring nodes until eventually converging on the most promising one. A higher branching factor will result in less certainty in the best path, a longer time until the algorithm converges and, in more extreme cases, the inability to properly explore certain nodes at all. However, it is still likely to find the best or at least a good move. This is in contrast to, for example, alpha-beta pruning, in which the time to run the algorithm will grow exponentially with the branching factor. This trait of MCTS is very useful for Risk, which has an extremely high branching factor \\textbf{CITATION?} - many positions have a branching factor in the tens or hundreds of millions. For comparison, the branching factor of chess is around 35, while Go has a factor of around 250.

\section{Using MCTS for Risk}
\label{MCTSforRisk}