\graphicspath{ {./Images/} }
\chapter{Background}
\label{background}
\section{Rules of Risk}
\label{rulesOfRisk}

Risk \cite{riskrules} is a 2-6 player strategy board game in which players try to conquer the world. The game is played on a game board representing the entire world (seen in Figure \ref{fig:RiskGameSet}), which is divided into 42 territories, with each of those territories belonging to one of 6 continents. Players start by controlling an equal amount of territories each, and must earn more troops and attack territories of other players in order to win.

\begin{figure}[H]
\includegraphics[width=0.9\textwidth]{RiskGameSet}
\caption{The Risk board game set}
\label{fig:RiskGameSet}
\end{figure}

A game starts by each player placing one troop on a territory, thereby claiming it. Once every territory is claimed, players continue placing troops on their already claimed territories, until each player has placed between 40 and 20 troops, depending on how many players there are. After this set-up phase is complete, players take their turns in sequence. A turn begins with the player distributing newly earned troops on their territories. The amount of new troops they earn is predetermined based on territories and continents - the player gets 1 troop for every 3 territories they control, rounded down, with a minimum of 3 troops, even if they control less than 9 territories. Furthermore, the player gets extra troops if they control every territory in a continent, with the exact number they get depending on the specific continent - ranging from 2 troops for controlling Australia up to 7 troops for controlling Asia.

After their new troops are deployed, the player can start making attacks. Territories can be attacked from adjacent territories that contain more than 1 troop. It is never allowed, under any circumstances, to leave a territory with 0 troops. For example, when trying to attack from a territory with 8 troops, only 7 troops can participate in the attack - there must always be at least 1 troop that is left in a territory. 

The attack itself is decided via dice roll. The attacker rolls 3 dice or less, if they have less than 3 attacking troops. The defender rolls 2 dice or less, if they have less than 2 defending troops. After the dice are rolled, the highest and the second highest rolled dice from both players are matched. For each match, whoever has the higher roll makes the other lose 1 troop. Ties are won by the defender. For example, say an attacker rolled dice with values \texttt{5, 3, 2}, and the defender rolled \texttt{5, 1}. The highest rolls are a 5 for both players, so the 5's are matched against each other. The second highest rolls are 3 for the attacker, 1 for the defender. The first match is a tie, therefore, the defender wins, and the attacker loses 1 troop. In the second match, the attacker rolled higher, so the defender loses 1 troop. Final result: both players lose 1 troop. The attacker can choose to stop attacking after each dice roll is resolved, or can keep rolling dice until either player has no troops left to fight for that territory.

If an attack is successful - that is, if the defender loses all of their defending troops - the attacking player can move some of the troops they were attacking with into the newly captured territory. The minimum number of troops they have to move is the number of dice they rolled on their last attack (most likely, 3). The maximum number is the total number of attacking troops (keeping in mind that, once again, at least 1 troop must be left behind). 

The player can make any number of different attacks in their turn, until either there are no more legal attacks to make, or they decide to stop attacking. After a player decides to stop attacking, they can make one reinforcing move between two of their own (adjacent) territories, moving any number of troops between those two territories. Once this move is done, the turn ends, and the next player starts their turn - deploying new troops, attacking, then reinforcing their own territories. This continues until one of the players controls all 42 territories and wins.

If a player has captured at least 1 territory in their turn, at the end of said turn, they receive a card. There are 44 cards in total - 42 representing each territory on the board, and 2 "wild cards". Each non-wild card also has a picture on it - of either infantry, cavalry, or artillery. When deploying new troops at the start of their turn, a player may trade their cards in for extra troops. They can trade in a set of 3 cards, which must have either matching pictures or 3 different pictures. A wild card may be used in place of any picture type. If a player has 5 or more cards at the start of their turn, they must trade a set in (as it is guaranteed that they have a valid set).

The first set traded in gives 4 troops, and every set traded in by any player increases the reward of the next set to be traded in. Furthermore, if the player owns any of the territories represented by the cards that are being traded in, they receive 2 extra troops from that set. Finally, if a player is defeated - meaning they do not control any territories - their hand of cards is given to the player that captured their final territory.

\section{Monte Carlo Tree Search Theory}
\label{MCTSTheory}

Monte Carlo tree search (MCTS) \cite{Coulom2007MonteCarlo} is a search algorithm that - as the name suggests - utilizes the Monte Carlo method to explore a game tree. More specifically, MCTS builds a tree of moves by continually running simulations until a game concludes and keeping track of the results of those simulations, preferring to explore nodes that have a higher average simulation results. The MCTS algorithm starts from the current game state as the root of the tree, and continuously performs \textit{rollouts} from that state - until either a certain amount of time passes, or a certain amount of rollouts are performed. A rollout is the core feature of MCTS, and generally has 4 stages (illustrated in Figure \ref{fig:MCTSRollout}):
\begin{itemize}
\item \textbf{Select} - the algorithm travels from the root to an unexplored leaf node. The chosen path/leaf node depends on the cumulative simulation values of the nodes, and how many times a node has been visited. The algorithm needs to strike a balance between exploration of nodes that have not been simulated many times, and exploitation of nodes that have high potential. The most common algorithm to use for selection is UCT \cite{kocsis2006bandit}:

\[UCT(i)=\frac{W_i}{n_i}\\+c\sqrt{\frac{\ln(N_i)}{n_i}}\]

where:
\begin{itemize} 
\item \(i = \) The node being evaluated
\item \(W_i = \) Cumulative heuristic value of the node
\item \(n_i = \) Number of rollouts done through the node
\item \(c = \) A constant. Theoretically should be \(\sqrt{2}\), but usually decided empirically
\item \(N_i = \) The number of rollouts performed through the \textit{parent} of the current node
\end{itemize}

The selection process selects a node with the highest UCT value until a leaf node is reached.
\item \textbf{Expand} - The algorithm creates a new leaf node from the selected node by picking a random move out of all the possible moves the player can make in that node and adding the game state resulting from that move as a new node.
\item \textbf{Simulate} - From the newly expanded node, a simulation is run until the end of the game, and the result is recorded - 1 if the player won, 0 if they lost. For games with a depth too large to realistically simulate many times, the simulation can cut off at a certain depth and perform a heuristic evaluation, returning a value between 0 and 1.
\item \textbf{Backpropagate} - the result of the simulation is backpropagated up the tree. Starting from the new node - increase the "number of simulations" counter by 1, increase the cumulative simulation value by the result of the simulation (0/1/heuristic value), go to the parent of the node. Repeat until the root node is reached.
\end{itemize}

\begin{figure}[H]
\includegraphics[width=\textwidth]{MCTSRollout}
\caption{A diagram showing the MCTS rollout process}
\label{fig:MCTSRollout}
\end{figure}

The MCTS algorithm has several advantages over other search approaches such as alpha-beta pruning \cite{knuth1975analysis}. The first one is that the "pure" version of MCTS (with simulating until game end, rather than cut-off and heuristic evaluation) requires no game knowledge - game states are evaluated based on how many simulations were won from that state, rather than a heuristic that is based on expert game knowledge. Some understanding of the game rules is required to make a viable simulation, however, the simulation can - and should - be very rudimentary, as it needs to be computationally cheap. For example, a possible strategy for simulations for chess could be "capture pieces when possible". This is useful, as unsolved games - games where even the best expert human players do not have complete knowledge and do not play the game fully optimally - are usually the most interesting ones to make computer players for.

The second advantage of MCTS is its ability to deal with games with a high branching factor. The MCTS algorithm focuses on nodes that have both a high cumulative simulation value (meaning the simulations that have gone through that node have a high chance of resulting in a win) and a high number of playthroughs (meaning the algorithm has a high certainty for the accuracy of the simulation value). The algorithm will usually follow a course of exploring nodes until eventually converging on the most promising one. A higher branching factor will result in less certainty in the best path, a longer time until the algorithm converges and, in more extreme cases, the inability to properly explore certain nodes at all. However, it is still likely to find the best or at least a good move. This is in contrast to alpha-beta pruning, in which the time to run the algorithm will grow exponentially with the branching factor. This trait of MCTS is very useful for Risk, which has an extremely high branching factor \cite{bauer2023artificial} - many positions have a branching factor in the tens or hundreds of millions. For comparison, the branching factor of chess is around 35, while Go has a factor of around 250.

Due to these advantages, MCTS is widely used in creating computer players \cite{swiechowski2023monte} for games such as Go \cite{silver2016mastering,coulom2007computing}, Magic: the Gathering \cite{cowling2012ensemble}, and even Super Mario Bros. \cite{jacobsen2014monte}. While most of these games have little in common with \textit{Risk}, it shows that this algorithm has high potential in both power and different ways to apply it. Combined with the fact that its strengths (particularly its relatively high branching factor tolerance) are particularly suited for \textit{Risk}, this made MCTS the clear best choice for a search-based approach.

\section{Using MCTS for Risk}
\label{MCTSforRisk}
Monte Carlo tree search is a popular approach to making computer players for both board and digital games, and Risk is one of the most popular board games in the world. It may, therefore, come as a surprise to learn that there is not a lot of research into how to use MCTS for a computer player for Risk. The most direct attempt to combine these two concepts is by \cite{limer2020monte}. This piece of research contains some useful information for an approach to Risk with Monte Carlo tree search, such as suggestions for action pruning and what \texttt{c} value to choose for the UCT algorithm \cite{kocsis2006bandit}. However, it also has some major shortcomings - the context of publication is unclear (besides the fact it was done for NATO), they do not provide source code, pseudocode, or any detailed description of how specifically they used MCTS for Risk, and the only evaluation of their algorithm is the win percentage against the researchers (without indication of the skill of said researchers), even noting in the paper that "the data is very sparse".

With research directly combining MCTS and Risk being sparse, data for creating such an approach must come from indirect sources - research that does not look specifically into using MCTS for Risk. Risk has a lot of complications which are not part of the default, "pure" way to run MCTS - such as high reliance on dice rolls, imperfect information (in regards to what player cards other players have), and the fact that each full turn of a player consists of multiple stages and multiple decision points. It is therefore useful to look at research that looks into these problems individually.

The problem of multiple moves in a single turn is investigated by Kowalski et al. \cite{kowalski2022split}, which looks into how to use MCTS for an abstract board game which uses split moves. This paper suggests using each individual action that a player can take (in case of Risk, individual actions would be an attack or a reinforcing move) as a node in the tree. This is in contrast to traditional MCTS, where a node is intended to be a 'ply' - that is, the full sequence of actions a player takes from the start of their turn to the end. The classic approach fits games such as chess, where a player's move consists of only one action, but is not sufficient at all in games like Risk, where it results in the aforementioned extreme branching factor. This makes using some sort of split-move approach for Risk almost required.

