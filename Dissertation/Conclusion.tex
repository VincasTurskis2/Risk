\graphicspath{ {./Images/} }
\chapter{Conclusion}
\label{conclusion}
\section{Review of Aims}
\label{reviewOfAims}

There were two aims I had previously laid out for this project - implementing a game client, and implementing an MCTS-based computer player. In my opinion, both aims have been achieved, although neither has been realized to its full potential.

For the aim of making a game client, it is fully functional, implements all the rules of (traditional) \textit{Risk}, and allows the human player to play against computer players and other human players on the same device. However, it is quite lacking in usability - there is no tutorial or instruction on how to play \textit{Risk}, there is no indication what will happen if a player clicks on territories, and little explanation how computer players make their moves or how long making a move will take for them.

As for the computer player, I have successfully written an agent that utilizes a hybrid of MCTS and rule-based approaches to choose and make moves on the board. The performance of this agent was initially not good enough to be considered a success, however, as discussed in Chapter \ref{evaluation}, the last-minute breakthrough elevated it to something that clearly has logic behind its moves, and showcases some emergent strategies - with a particular tendency to start turtling if it is losing. However, this agent is inefficient, and takes too much time to find good strategies. It also lacks a thorough evaluation, so its exact strength against human players is unknown.

\section{Learning Process}
\label{learningProcess}

I learned quite a lot while working on this project. I believe that, for me, it was in the sweet spot of learning, containing both familiar aspects and concepts and completely new areas of research. I had already had some experience in making games and using Unity, which helped me tremendously in the early stages of the project. However, the other part of it - making a computer player - was novel to me, and I had to build my knowledge up from scratch. Overall, I would classify the things I have learned during this project into 3 categories:
\begin{itemize}
\item \textbf{Technical knowledge - } The tangible skills and know-how I picked during my work are varied. I am now far more knowledgeable in search-based approaches to computer players, particularly Monte Carlo tree search. I learned the strengths, weaknesses and applications of MCTS, along with several other search based approaches, such as alpha-beta pruning.

Besides my research into AI, I had quite a lot to learn about different ways to work with Unity and C\#. This project required me to structure my code somewhat against Unity's design - such as avoiding using \texttt{MonoBehaviour} classes in order to not be fully reliant on the Unity client for things like MCTS rollout simulations. As for C\#, the most useful coding skill I learned was Linq notation, which allows for functional programming-like operations on collections. Since discovering it, I have used it extensively both within this project and outside of it.
\item \textbf{Decision making - } Besides the specific technical things I have learned about, I have also gained experience in less tangible skills, such as ability to plan and structure large projects. I started work on this project from, in my opinion, the wrong direction, making the game client fully before the computer player. I attempted to make the design of the code extendible and flexible, with the intention of making the computer player easy to implement on top. This would have been much more likely to succeed had I done more research into possible computer players beforehand. As it stands, I failed in my goal, and had to undergo major refactoring of my code, as discussed in previous chapters. Learning more about design patters and other ways to structure code at that point would also been very useful, as the final version of the code is somewhat disorganized and difficult to work with and debug.
\item \textbf{Soft skills - } The final aspect of learning, and arguably the most important, is the things I learned about working itself. Time management, stress management, self-discipline and ability to research are all things I improved on during this project, although I am by no means perfect with any of them. The project also showcased the need for these skills very well. For example, had I managed my time better, I could have had more of it to react to the sudden breakthrough in the performance of the MCTS player, and performed a more thorough evaluation. Overall, I consider these to be my most important takeaways from the project, as while I might not end up working with Unity or MCTS again, the soft skills that were honed by this project will be useful in every aspect of life.
\end{itemize}

\section{Future Work}
\label{futureWork}

There is plenty of future work that could be done for this project, ranging from minor fixes and improvements to major feature additions. Once again, it is best to split the discussion into improvements of the game client and of the computer player.

The game client can be improved in 3 main ways - user experience, optimization, and flexibility. The user experience of the current version is rather poor for anyone that is not a developer of the client. Potential improvements here are endless - from adding more feedback to player actions, to adding sound and music, to introducing a consistent art style and theme, to adding a tutorial.

In terms of optimization, it is likely a necessary condition to optimize the client code in order to make improvements to the computer player. The current code structure is not efficient, particularly in the case of cloning a game state, which has a major impact on performance for the MCTS player. Another refactoring should be done in order to bring the performance of the code closer to optimal and to provide the initially planned for ability to easily add new computer players that utilize different approaches.

Finally, the current client has some values hard-coded, such as the parameters of the computer players, the number of games the evaluation system will run, the type of data it will output, and others. Adding these as options in the user interface would make the lives of users - both those looking to play the game and those looking to test their computer players - much easier.

As for the computer player, the main and most realistic avenue of improvement would be to conduct a more thorough evaluation. The empirically correct value of \textit{c} for UCT could be found by running multiple games with versions of the agent with different \textit{c} values playing against each other. Similar evaluation could measure the impact of the number of rollouts and the depth of simulation on performance of the computer player, and find the point where the extra cost of computation outweighs the benefits provided by increasing these values. The system to run this evaluation is already in place, however, I could not utilize it to its fullest due to the aforementioned late performance breakthrough.

After fine-tuning those values, further research could evaluate the rule-based aspects of this computer player. Some of the rules used by it were added under false assumptions about the algorithm's performance, such as the continent-based deployment rule. It's a strong possibility that removing these rules, and replacing them with random selection or even letting MCTS include them in the game tree could lead to a stronger computer player. The heuristic applied to positions in the simulation step of MCTS could also be evaluated.

I believe these improvements to the computer player have the highest potential to add something to the broader sphere of artificial intelligence and computer players of games. In my opinion, the rule-based/MCTS hybrid approach has high potential, and could be applied not only to Risk, but to other games as well. Rules can be applied to the more straight-forward parts of games, freeing up time and computational power for the MCTS part to search for solutions to the complex parts. With research broadly moving on to complex and general AI approaches such as deep learning, I think the simpler, rule- and search- based approaches still have more to offer.