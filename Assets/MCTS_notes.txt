Things to ask fabio:
if the UCB1 formula selects a leaf that already exists, should i run the UCB1 formula again on that leaf? (Probably yes)
Ideas for a playout policy?
MCTS returns an action - how to do that in OOP/C#? Delegates? Func<>? Make an action into a class or interface

Multiply the heuristic value of a state by the probability of getting to that state for the purposes of backpropagating the value

rule-based for troop placement and reinforcement
1 mtcs search for the attacks.

finding possible attacks and executing them are different!
Can have a bunch of checks and decisions during execution in case something goes wrong

MCTS(State state){
    new tree;
    tree.addNode(state, {root})
    while isTimeRemaining(){
        State leaf = select(tree)
        State child = expand(leaf)
        tree.addNode(child, leaf)
        result = simulate(child)
        back_propagate(result, child)
    }
    return {Action that has the highest number of playouts}
}

needed for this - a way to represent a state (that could be copied)?, selection policy, playout policy, heuristic

playout policy - some simple decision making process for making moves. Could be "attack if possible" or "place troops near the border". Could also be random
more possibilites - attack when much more attackers than defenders

possible good selection policy - "upper confidence bounds applied to trees" ("UCT"). 
It ranks all moves based on an upper confidence bound formula called UCB1. For node n:

UCB1(n) = U(n)/N(n) + C * root(log N(Parent(n))/N(n))

U(n) - total utility of all playouts that went through n
N(n) - number of playouts that went through N
C - a constant. usually square root of 2, but can be experimented with

run this formula on all possible moves from a child node, select the node with the balanced result. If it already exists, run the formula again(?)
on the child node. If not, run create a new node and run a simulation choosing moves based on the playout policy

finally, update all parent nodes with the information.



Making some assumptions:
- The placement of troops and reinforcement will be rule-based
    - Possible rule - place them on the territory with the most enemy troops touching it


Fabio gave me a bunch of papers. One of them is "Split moves for monte-carlo tree search", Kowalski et al. It contains pseudocode for split-move mtcs. Going to try to follow that

Kinda failed following that.

Another paper - "Monte Carlo Tree Search for Risk", Limer, Kalmer, Cohen, has some tips about simplifying the search. I'm also trying to use the tips in here.

Problem - simulation does not reach the end, seemingly goes in circles of capturing-recapturing territories.
Need to have AI pay attention to continents - deploy troops in continents that they are strong in, and then look for non-suicidal attacks

Will likely have to use RAVE, as there might be a problem of same permutations taking up space. Or not, because i don't have time.

What to try in order to have a good simulation:
1st idea: after a simulated battle, only the loser of the battle loses troops

2nd idea:
- Make a static class strategies.cs, which has a bunch of static methods (take player as an argument) on how to make deploy/attack/reinforce troops. Move currently used strategies to those methods
- Add new deploy strategy - rank continents based on (own troop no in continent+adjacent lands/total troop no in continent+adjacent lands) in that continent. Deploy on the border in the highest ranked continent. If the highest-ranked continent does not have any borders - try the next highest, etc.
- Add new attack strategy - rank continents again, try random attack whose target is in the highest ranked continent. If impossible - try next highest, etc.
- (Maybe) add the limitation that only attacks where you have the advantage are considered (remember that defender loses more troops than attacker, see py script)
- Use the new deploy strategy in both simulation and actual deploy, use new attack strategy in simulation
- Adjust the simulation troop loss count to take the py script ratio into account.

If this doesn't work, Give up, describe failure in report.
    